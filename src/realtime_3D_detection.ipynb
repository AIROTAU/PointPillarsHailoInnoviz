{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time 3D Object Detection with PointPillars and Hailo Integration\n",
    "\n",
    "This notebook demonstrates how to create a real-time inference pipeline for 3D object detection using the PointPillars network from the OpenPCDet repository, with acceleration from a Hailo device. By offloading the heavy 2D convolutional computations to the Hailo hardware, we can significantly improve inference speed suitable for real-time applications.\n",
    "This notebook is based on Hailo's script available in their application code examples: [Hailo Application Code Examples](https://github.com/hailo-ai/Hailo-Application-Code-Examples/tree/main).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup](#setup)\n",
    "   - [Install Dependencies](#install-dependencies)\n",
    "   - [Import Libraries and Set Paths](#import-libraries-and-set-paths)\n",
    "3. [Load and Prepare the Model](#load-and-prepare-the-model)\n",
    "   - [Load Configuration and Build the Model](#load-configuration-and-build-the-model)\n",
    "   - [Run a Sanity Test](#run-a-sanity-test)\n",
    "4. [Integrate with Hailo Hardware](#integrate-with-hailo-hardware)\n",
    "   - [Export the 2D Backbone and Detection Head to ONNX](#export-the-2d-backbone-and-detection-head-to-onnx)\n",
    "   - [Translate ONNX to Hailo Format](#translate-onnx-to-hailo-format)\n",
    "   - [Verify Inference Equivalence with Hailo Emulation](#verify-inference-equivalence-with-hailo-emulation)\n",
    "5. [Optimize and Compile for Hailo Hardware](#optimize-and-compile-for-hailo-hardware)\n",
    "   - [Create Calibration Dataset](#create-calibration-dataset)\n",
    "   - [Run Model Optimization (Quantization)](#run-model-optimization-quantization)\n",
    "   - [Compile the Model for Hailo Hardware](#compile-the-model-for-hailo-hardware)\n",
    "6. [Run Inference with Hailo Offload](#run-inference-with-hailo-offload)\n",
    "7. [Conclusion](#conclusion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We will guide you through setting up the environment, preparing the data, integrating the PointPillars model with Hailo hardware, and running real-time inference on point cloud data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install Dependencies\n",
    "1. Install CUDA and PyTorch\n",
    "\n",
    "Ensure you have CUDA installed.   \n",
    "The following PyTorch versions have been tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# For CUDA 11.3\n",
    "pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "# For CUDA 10.2\n",
    "pip install torch==1.12.1+cu102 torchvision==0.13.1+cu102 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Clone and Install OpenPCDet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/open-mmlab/OpenPCDet.git\n",
    "cd OpenPCDet\n",
    "pip install -r requirements.txt\n",
    "pip install spconv kornia\n",
    "python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you haven't cloned this git repository yet, do it now.\n",
    "  \n",
    "Clone this git repo to your environment if you plan on using one of the files in it (e.g. model weights, the adjusted yaml configuration file for the Poinpillars model, custom dataset configuration for Innoviz pointclouds etc.). \n",
    " \n",
    "**NOTE** that even if you use your own weigts and model/dataset configurations, you still need to make sure you have the ``openpcdet2hailo_utils.py`` file as it is necessary for this pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Set Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Replace with your OpenPCDet clone directory\n",
    "openpcdet_clonedir = '/path/to/OpenPCDet'\n",
    "sys.path.append(openpcdet_clonedir + '/tools/')\n",
    "\n",
    "from pcdet.config import cfg, cfg_from_yaml_file\n",
    "from pcdet.utils import common_utils\n",
    "from pcdet.models import build_network, load_data_to_gpu\n",
    "from pcdet.datasets import DatasetTemplate\n",
    "\n",
    "# Import custom utilities for OpenPCDet and Hailo integration\n",
    "import openpcdet2hailo_utils as ohu;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare the Model\n",
    "### Load Configuration and Build the Model\n",
    "Specify the paths to the model configuration file, pretrained weights, and point cloud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to model configuration and weights\n",
    "yaml_name = '/path/to/yaml/cfg/file'\n",
    "pth_name = '/path/to/.pth/file'\n",
    "\n",
    "# Path to point cloud data\n",
    "sample_pointclouds = '/path/to/pc_samples/testing/innoviz'\n",
    "demo_pointcloud = '/path/to/pc_samples/testing/innoviz/00001.npy'\n",
    "\n",
    "# File extension of point cloud files\n",
    "pc_file_extention = '.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the configuration and build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = common_utils.create_logger()\n",
    "cfg_from_yaml_file(yaml_name, cfg)\n",
    "\n",
    "demo_dataset = ohu.DemoDataset(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    class_names=cfg.CLASS_NAMES,\n",
    "    training=False,\n",
    "    root_path=Path(sample_pointclouds),\n",
    "    ext=pc_file_extention,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "logger.info(f'Total number of samples: \\t{len(demo_dataset)}')\n",
    "\n",
    "# Build the model and load pretrained weights\n",
    "model = build_network(\n",
    "    model_cfg=cfg.MODEL,\n",
    "    num_class=len(cfg.CLASS_NAMES),\n",
    "    dataset=demo_dataset\n",
    ")\n",
    "model.load_params_from_file(filename=pth_name, logger=logger, to_cpu=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(cfg, pth_name, demo_dataset):    \n",
    "    model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=demo_dataset)\n",
    "    model.load_params_from_file(filename=pth_name, logger=logger, to_cpu=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def cfg_from_yaml_file_wrap(yaml_name, cfg):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(openpcdet_clonedir+'/tools/')\n",
    "    cfg_from_yaml_file(yaml_name, cfg)\n",
    "    os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Sanity Test\n",
    "Process a point cloud to ensure the model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_from_yaml_file_wrap(yaml_name, cfg)\n",
    "if False: # \n",
    "    display(cfg.DATA_CONFIG)\n",
    "\n",
    "demo_dataset = ohu.DemoDataset(\n",
    "    dataset_cfg=cfg.DATA_CONFIG, class_names=cfg.CLASS_NAMES, training=False,\n",
    "    root_path=Path(demo_pointcloud), ext=pc_file_extention, logger=logger\n",
    ")\n",
    "logger.info(f'Total number of samples: \\t{len(demo_dataset)}')\n",
    "\n",
    "model = get_model(cfg, pth_name, demo_dataset)\n",
    "model_cpu = ohu.PointPillarsCPU(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, data_dict in enumerate(demo_dataset):        \n",
    "        data_dict = demo_dataset.collate_batch([data_dict])\n",
    "        # pred_dicts, _ = model_cpu(data_dict)\n",
    "        pred_dicts = model_cpu(data_dict)        \n",
    "        break\n",
    "\n",
    "print(pred_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with Hailo Hardware\n",
    "We will offload the 2D backbone and detection head computations to the Hailo device.\n",
    "\n",
    "### Export the 2D Backbone and Detection Head to ONNX\n",
    "Extract the 2D convolutional parts and export them to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the module that includes backbone_2d and dense_head\n",
    "bev_w_head = ohu.Bev_w_Head(model.backbone_2d, model.dense_head)\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    bev_w_head,\n",
    "    args=(data_dict['spatial_features'],),\n",
    "    f=\"pp_bev_w_head.onnx\",\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify the ONNX model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onnxsim pp_bev_w_head.onnx pp_bev_w_head_simple.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "from hailo_sdk_client import ClientRunner\n",
    "from hailo_sdk_common.targets.inference_targets import SdkNative\n",
    "from hailo_sdk_client import InferenceContext #SdkPartialNumeric, SdkNative # \n",
    "import tensorflow as tf\n",
    "import hailo_sdk_client, hailo_sdk_common\n",
    "print(hailo_sdk_client.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "### Translate ONNX to Hailo Format\n",
    "Use the Hailo SDK to translate the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hailo_sdk_client import ClientRunner\n",
    "\n",
    "runner = ClientRunner(hw_arch='hailo8')\n",
    "\n",
    "onnx_path = \"pp_bev_w_head_simple.onnx\"\n",
    "hn, npz = runner.translate_onnx_model(onnx_path)\n",
    "\n",
    "# Save the translated model\n",
    "har_name = 'pp_bev_w_head.har'\n",
    "runner.save_har(har_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Inference Equivalence with Hailo Emulation\n",
    "Ensure the Hailo-emulated model produces similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bev_W_Head_Hailo(torch.nn.Module):\n",
    "    \"\"\" Drop-in replacement to the sequence of original \"backbone-2d\" and \"dense_head\" modules, accepting and returning dictionary,\n",
    "        while under the hood using Hailo [emulator] implementation for the 2D CNN part, accepting and returning tensors I/O\n",
    "    \"\"\"\n",
    "    def __init__(self, runner, emulate_quantized=False, use_hw=False, generate_predicted_boxes=None):\n",
    "        super().__init__()\n",
    "        self._runner = runner\n",
    "        self.generate_predicted_boxes = generate_predicted_boxes\n",
    "        \n",
    "        if use_hw:\n",
    "            context_type = InferenceContext.SDK_HAILO_HW\n",
    "        elif emulate_quantized:\n",
    "            context_type = InferenceContext.SDK_QUANTIZED \n",
    "        else:\n",
    "            context_type = InferenceContext.SDK_FP_OPTIMIZED\n",
    "            \n",
    "        with runner.infer_context(context_type) as ctx:\n",
    "            self._hailo_model = runner.get_keras_model(ctx)   \n",
    "            \n",
    "    def forward(self, data_dict):        \n",
    "        spatial_features = data_dict['spatial_features']\n",
    "        \n",
    "        spatial_features_hailoinp = np.transpose(spatial_features.cpu().detach().numpy(), (0,2,3,1))\n",
    "        \n",
    "        # ============ Hailo-emulation of the Hailo-mapped part ==========\n",
    "        spatial_features_2d, cls_preds, box_preds, dir_cls_preds = \\\n",
    "                            self._hailo_model(spatial_features_hailoinp)\n",
    "        # ================================================================\n",
    "        \n",
    "        print(cls_preds.shape, type(cls_preds), box_preds.shape)\n",
    "        cls_preds = torch.Tensor(cls_preds.numpy()) # .permute(0, 2, 3, 1).contiguous()          # [N, H, W, C]\n",
    "        box_preds = torch.Tensor(box_preds.numpy()) # .permute(0, 2, 3, 1).contiguous()          # [N, H, W, C]\n",
    "        dir_cls_preds = torch.Tensor(dir_cls_preds.numpy()) # .permute(0, 2, 3, 1).contiguous()\n",
    "                \n",
    "        data_dict['spatial_features_2d'] = torch.Tensor(spatial_features_2d.numpy())\n",
    "        \n",
    "        batch_cls_preds, batch_box_preds = self.generate_predicted_boxes(\n",
    "            batch_size=data_dict['batch_size'],\n",
    "            cls_preds=cls_preds, box_preds=box_preds, dir_cls_preds=dir_cls_preds\n",
    "        )\n",
    "        data_dict['batch_cls_preds'] = batch_cls_preds\n",
    "        data_dict['batch_box_preds'] = batch_box_preds\n",
    "        data_dict['cls_preds_normalized'] = False\n",
    "\n",
    "        return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test(runner, hailoize=True, emulate_quantized=False, use_hw=False, fname='00001.npy', verbose=False):\n",
    "    \"\"\" Encapsulates a minimalistic test of the complete network with/without hailo offload emulation \n",
    "    \"\"\"\n",
    "    demo_dataset = ohu.DemoDataset(\n",
    "        dataset_cfg=cfg.DATA_CONFIG, class_names=cfg.CLASS_NAMES, training=False,\n",
    "        root_path=Path(fname), ext=pc_file_extention, logger=logger\n",
    "    )\n",
    "    logger.info(f'Total number of samples: \\t{len(demo_dataset)}')\n",
    "    \n",
    "    model_h = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=demo_dataset)\n",
    "    model_h.load_params_from_file(filename=pth_name, logger=logger, to_cpu=True)\n",
    "    model_h.eval()\n",
    "\n",
    "    #bb2d_hailo1 = BB2d_Hailo(runner, pppost_onnx='./pp_tmp_post.onnx', emulate_quantized=emulate_quantized, use_hw=use_hw)\n",
    "    bev_w_head_hailo = Bev_W_Head_Hailo(runner, generate_predicted_boxes=model_h.dense_head.generate_predicted_boxes,\n",
    "                                        emulate_quantized=emulate_quantized, use_hw=use_hw)    \n",
    "    \n",
    "    if hailoize:        \n",
    "        # ==== Hook a call into Hailo by replacing parts of sequence by our rigged submodule ====\n",
    "        model_h.module_list = model_h.module_list[:2] + [bev_w_head_hailo]\n",
    "        # =======================================================================================                                          \n",
    "    \n",
    "    model_cpu = ohu.PointPillarsCPU(model_h) \n",
    "    logger.info(f'Total number of samples: \\t{len(demo_dataset)}')\n",
    "\n",
    "    for idx, data_dict in enumerate(demo_dataset):\n",
    "        logger.info(f'Visualized sample index: \\t{idx + 1}')\n",
    "        data_dict = demo_dataset.collate_batch([data_dict])            \n",
    "        # pred_dicts, _ = model_cpu.forward(data_dict)\n",
    "        pred_dicts = model_cpu.forward(data_dict)\n",
    "        if verbose:\n",
    "            print(pred_dicts)\n",
    "        else:\n",
    "            print(pred_dicts[0][0]['pred_scores'][:7])\n",
    "            \n",
    "quick_test(runner, hailoize=False, emulate_quantized=False)     \n",
    "quick_test(runner, hailoize=True, emulate_quantized=False)     \n",
    "# This should give exact same result as we're yet to actually emulate the HW datapath,\n",
    "# with its \"lossy-compression\" (e.g., 8b) features. \n",
    "# This will be possible after calibration and quantization of the model which will also enabling compilation for a physical HW.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize and Compile for Hailo Hardware\n",
    "### Create Calibration Dataset\n",
    "Prepare a dataset for quantizing the model.  \n",
    "Make sure you provide a path to at least 8 point cloud samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_from_yaml_file_wrap(yaml_name, cfg)\n",
    "demo_dataset = ohu.DemoDataset(\n",
    "    dataset_cfg=cfg.DATA_CONFIG, class_names=cfg.CLASS_NAMES, training=False,\n",
    "    root_path=Path(sample_pointclouds), ext=pc_file_extention, logger=logger\n",
    ")\n",
    "logger.info(f'Total number of samples: \\t{len(demo_dataset)}')\n",
    "model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=demo_dataset)\n",
    "model.load_params_from_file(filename=pth_name, logger=logger, to_cpu=True)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "cs_size = 8\n",
    "calib_set = np.zeros((cs_size, 496, 432, 64))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "perc4stats = [50, 90, 98.6, 99.7, 99.9]\n",
    "with torch.no_grad():\n",
    "    for idx, _data_dict in enumerate(demo_dataset):        \n",
    "        print(f\"cloud #{idx}\")\n",
    "        if idx >= cs_size:\n",
    "            break\n",
    "        _data_dict = demo_dataset.collate_batch([_data_dict])\n",
    "        load_data_to_gpu(_data_dict)\n",
    "        # pred_dicts, goo = model.forward(_data_dict) \n",
    "        pred_dicts = model.forward(_data_dict)       \n",
    "        calib_set[idx] = np.transpose(_data_dict['spatial_features'].cpu().numpy(), (0,2,3,1))\n",
    "        # Basic stats just to verify there's some data diversity (just in top percentile apparently...)\n",
    "        print(f'basic stats - percentile {perc4stats} of data (@ 2d-net input)', \\\n",
    "              np.percentile((np.abs(calib_set[idx])), perc4stats))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('calib.npy', calib_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Optimization (Quantization)\n",
    "Optimize and quantize the model using the Hailo SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration dataset\n",
    "calib_set = np.load('calib.npy')\n",
    "\n",
    "# Optimize the model\n",
    "runner.optimize(calib_set)\n",
    "\n",
    "# Save the quantized model\n",
    "q_har_name = 'pp_bev_w_head.q.har'\n",
    "runner.save_har(q_har_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model for Hailo Hardware\n",
    "Compile the quantized model to generate a HEF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quantized model\n",
    "runner = ClientRunner(har_path=q_har_name)\n",
    "\n",
    "# Compile the model\n",
    "compiled_model = runner.compile()\n",
    "\n",
    "# Save the compiled model\n",
    "hef_name = 'pp_bev_w_head.hef'\n",
    "with open(hef_name, 'wb') as f:\n",
    "    f.write(compiled_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_test(runner, hailoize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_test(runner, hailoize=True, emulate_quantized=True)\n",
    "\n",
    "# You should see here results with small differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference with Hailo Offload\n",
    "Set up Hailo Runtime to run inference with the Hailo device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "do_compile = True\n",
    "if do_compile:\n",
    "    alls_line1 = 'shortcut_concat1_conv20 = shortcut(concat1, conv20)\\n'\n",
    "    open('helper.alls','w').write(alls_line1)  #   !!!!\n",
    "    runner.load_model_script('./helper.alls') \n",
    "\n",
    "    compiled_model=runner.compile()    \n",
    "    open(hef_name, 'wb').write(compiled_model)\n",
    "\n",
    "# Expected results:\n",
    "# [info] | Cluster   | Control Utilization | Compute Utilization | Memory Utilization |\n",
    "# [info] +-----------+---------------------+---------------------+--------------------+\n",
    "#                                        ...\n",
    "# [info] +-----------+---------------------+---------------------+--------------------+\n",
    "# [info] | Total     | 62.5%               | 63.7%               | 37.2%              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "from hailo_platform import (HEF, PcieDevice, VDevice, HailoStreamInterface, ConfigureParams,\n",
    " InputVStreamParams, OutputVStreamParams, InputVStreams, OutputVStreams, FormatType)\n",
    "\n",
    "def send_from_queue(configured_network, read_q, num_images, start_time):\n",
    "    \"\"\" Bridging a queue into Hailo platform FEED. To be run as a separate process. \n",
    "        Reads (preprocessed) images from a given queue, and sends them serially to Hailo platform.        \n",
    "    \"\"\"    \n",
    "    configured_network.wait_for_activation(1000)\n",
    "    vstreams_params = InputVStreamParams.make(configured_network, quantized=False, format_type=FormatType.FLOAT32)\n",
    "    print('Starting sending input images to HW inference...\\n')\n",
    "    with InputVStreams(configured_network, vstreams_params) as vstreams:\n",
    "        vstream_to_buffer = {vstream: np.ndarray([1] + list(vstream.shape), dtype=vstream.dtype) for vstream in vstreams}\n",
    "        for i in range(num_images):\n",
    "            hailo_inp = read_q.get()\n",
    "            for vstream, _ in vstream_to_buffer.items():                                \n",
    "                vstream.send(hailo_inp)\n",
    "            print(f'sent img #{i}')\n",
    "    print(F'Finished send after {(time.time()-start_time) :.1f}')\n",
    "    return 0\n",
    "\n",
    "def recv_to_queue(configured_network, write_q, num_images, start_time):\n",
    "    \"\"\" Bridging Hailo platform OUTPUT into a queue. To be run as a separate process. \n",
    "        Reads output data from Hailo platform and sends them serially to a given queue.\n",
    "    \"\"\"\n",
    "    configured_network.wait_for_activation(1000)\n",
    "    vstreams_params = OutputVStreamParams.make_from_network_group(configured_network, quantized=False, format_type=FormatType.FLOAT32)\n",
    "    print('Starting receving HW inference output..\\n')\n",
    "    with OutputVStreams(configured_network, vstreams_params) as vstreams:\n",
    "        # print('vstreams_params', vstreams_params)\n",
    "        for i in range(num_images):            \n",
    "            hailo_out = {vstream.name: np.expand_dims(vstream.recv(), 0) for vstream in vstreams}    \n",
    "            \n",
    "            print(\"hailo_out keys:\", hailo_out.keys())\n",
    "                      \n",
    "            write_q.put(hailo_out)\n",
    "            print(f'received img #{i}')\n",
    "    print(F'Finished recv after {time.time()-start_time :.1f}')\n",
    "    return 0\n",
    "\n",
    "def generate_data_dicts(demo_dataset, num_images, pp_pre_bev_w_head):\n",
    "    for idx, data_dict in enumerate(demo_dataset):\n",
    "        if idx > num_images:\n",
    "            break\n",
    "        data_dict = demo_dataset.collate_batch([data_dict])\n",
    "        ohu.load_data_to_CPU(data_dict)\n",
    "        # Add sample_name to data_dict with only the file name\n",
    "        data_dict['sample_name'] = os.path.basename(demo_dataset.sample_file_list[idx])\n",
    "        # ------ (!) Applying torch PRE-processing -------\n",
    "        data_dict = pp_pre_bev_w_head.forward(data_dict)\n",
    "        # ------------------------------------------------\n",
    "        logger.info(f'preprocessed sample #{idx}')\n",
    "        yield data_dict\n",
    "\n",
    "def generate_hailo_inputs(demo_dataset, num_images, pp_pre_bev_w_head):\n",
    "    \"\"\" generator-style encapsulation for preprocessing inputs for Hailo HW feed\n",
    "    \"\"\"\n",
    "    for data_dict in generate_data_dicts(demo_dataset, num_images, pp_pre_bev_w_head):\n",
    "        spatial_features = data_dict['spatial_features']\n",
    "        spatial_features_hailoinp = np.transpose(spatial_features.cpu().detach().numpy(), (0, 2, 3, 1))\n",
    "        yield data_dict, spatial_features_hailoinp\n",
    "\n",
    "def post_proc_from_queue(recv_queue, num_images, pp_post_bev_w_head,\n",
    "                         output_layers_order=['model/concat1', 'model/conv19', 'model/conv18', 'model/conv20']):\n",
    "    results = []\n",
    "    for i in range(num_images):\n",
    "        t_ = time.time()\n",
    "        while(recv_queue.empty() and time.time()-t_ < 3):\n",
    "            time.sleep(0.01)\n",
    "        if recv_queue.empty():\n",
    "            print(\"RECEIVE TIMEOUT!\")\n",
    "            break\n",
    "        hailo_out = recv_queue.get(0)\n",
    "        bev_out = (hailo_out[lname] for lname in output_layers_order)\n",
    "        \n",
    "        # ------ (!) Applying torch POST-processing -------\n",
    "        pred_dicts, _ = pp_post_bev_w_head(bev_out)\n",
    "        # pred_dicts = pp_post_bev_w_head(bev_out)\n",
    "        # ------------------------------------------------\n",
    "        # Add sample_name to each prediction dictionary\n",
    "        sample_name = recv_queue.sample_names[i]\n",
    "\n",
    "        # Add 'sample_name' to the dictionary\n",
    "        pred_dicts['sample_name'] = sample_name\n",
    "\n",
    "        # Append the dictionary to results\n",
    "        results.append(pred_dicts)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set ``num_images`` to the number of sample you are going to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, onnxruntime\n",
    "\n",
    "data_source = demo_pointcloud  # replace by a folder for a more serious test\n",
    "num_images = 1\n",
    "\n",
    "cfg_from_yaml_file_wrap(yaml_name, cfg)\n",
    "logger = common_utils.create_logger()\n",
    "demo_dataset = ohu.DemoDataset(\n",
    "    dataset_cfg=cfg.DATA_CONFIG, class_names=cfg.CLASS_NAMES, training=False,\n",
    "    root_path=Path(data_source), ext=pc_file_extention, logger=logger\n",
    ")\n",
    "model = get_model(cfg, pth_name, demo_dataset)\n",
    "\n",
    "# Library creates the anchors in cuda by default (applying .cuda() in internal implementation)\n",
    "model.dense_head.anchors = [anc.cpu() for anc in model.dense_head.anchors]\n",
    "\n",
    "\"\"\" (!) Slicing off the torch model all that happens before and after Hailo\n",
    "\"\"\"\n",
    "\n",
    "pp_pre_bev_w_head = ohu.PP_Pre_Bev_w_Head(model)\n",
    "pp_post_bev_w_head = ohu.PP_Post_Bev_w_Head(model)\n",
    "    \n",
    "with VDevice() as target:\n",
    "    hef = HEF(hef_name)\n",
    "    configure_params = ConfigureParams.create_from_hef(hef, interface=HailoStreamInterface.PCIe)\n",
    "    network_group = target.configure(hef, configure_params)[0]\n",
    "    network_group_params = network_group.create_params()\n",
    "    recv_queue = Queue()\n",
    "    send_queue = Queue()\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    hw_send_process = Process(target=send_from_queue, args=(network_group, send_queue, num_images, start_time))\n",
    "    hw_recv_process = Process(target=recv_to_queue, args=(network_group, recv_queue, num_images, start_time))\n",
    "\n",
    "    sample_names = []\n",
    "\n",
    "    with network_group.activate(network_group_params):\n",
    "        hw_recv_process.start()\n",
    "        hw_send_process.start()\n",
    "\n",
    "        tik = time.time()\n",
    "\n",
    "        for data_dict, hailo_inp in generate_hailo_inputs(demo_dataset, num_images, pp_pre_bev_w_head):\n",
    "            send_queue.put(hailo_inp)\n",
    "            sample_names.append(data_dict['sample_name'])\n",
    "        \n",
    "        recv_queue.sample_names = sample_names\n",
    "\n",
    "        results = post_proc_from_queue(recv_queue, num_images, pp_post_bev_w_head)\n",
    "\n",
    "        # Stop timing after processing\n",
    "        tok = time.time()\n",
    "\n",
    "        elapsed_time = tok - tik\n",
    "        average_time_per_image = elapsed_time / num_images\n",
    "        inference_rate_hz = num_images / elapsed_time\n",
    "\n",
    "        print(f\"Total elapsed time: {elapsed_time:.4f} seconds\")\n",
    "        print(f\"Average time per image: {average_time_per_image:.4f} seconds\")\n",
    "        print(f\"Inference rate: {inference_rate_hz:.2f} Hz\")\n",
    "                             \n",
    "    hw_recv_process.join(10)\n",
    "    hw_send_process.join(10)\n",
    "    \n",
    "    pred_dicts = results[-1]\n",
    "    print(pred_dicts['pred_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we demonstrated how to set up a real-time inference pipeline for 3D object detection using the PointPillars network with acceleration from a Hailo device. By offloading heavy computations to the Hailo hardware, we achieved improved inference speed suitable for real-time applications.\n",
    "\n",
    "---\n",
    "\n",
    "Note: Ensure all custom utilities (openpcdet2hailo_utils.py and any other required modules) are properly imported and available in your environment. Adjust file paths and configurations according to your setup. Some sections, especially the Hailo integration parts, may require additional implementation details based on your specific hardware and software environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcdet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
