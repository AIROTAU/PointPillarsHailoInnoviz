{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D detector evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "#### File Paths:\n",
    "\n",
    "Ensure that `base_dir` is correctly set to the path where your data is located.\n",
    "\n",
    "The script assumes the following directory structure:\n",
    "\n",
    "<pre>\n",
    "base_dir/\n",
    "├── detector_labels/\n",
    "├── gt_test/\n",
    "│   ├── labels/\n",
    "│   ├── npy/\n",
    "│   └── html/\n",
    "</pre>\n",
    "\n",
    "#### Dependencies:\n",
    "\n",
    "The script uses `numpy`, `glob`, `os`, and `plotly`.\n",
    "\n",
    "Ensure you have `plotly` installed (`pip install plotly`) and that `BoundingBox3D` is properly defined in `bbox_utils`.\n",
    "\n",
    "#### Visualization in Notebook:\n",
    "\n",
    "When `show_in_notebook` is `True`, the visualization will be displayed inline if you're running in a Jupyter notebook.\n",
    "\n",
    "If running in a script or console, this may not display, and you might need to adjust the code accordingly.\n",
    "\n",
    "#### Saving Visualizations:\n",
    "\n",
    "If `save_visualizations` is `True`, the script will save HTML files for each visualization.\n",
    "\n",
    "The files are saved in a subdirectory under `html_path`, named after each file's basename with a `_results` suffix.\n",
    "\n",
    "#### Adjusting Thresholds:\n",
    "\n",
    "You can adjust `confidence_threshold` and `iou_threshold` according to your evaluation criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the pickle results file into txt file:\n",
    "First - convert the results.pkl file in the output directory to txt files using get_results.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from bbox_utils import BoundingBox3D\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Data Loading Functions\n",
    "# =====================================================\n",
    "\n",
    "def read_gt_file(fname):\n",
    "    \"\"\"\n",
    "    Reads ground truth bounding boxes from a text file.\n",
    "    Each line in the file corresponds to one bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        fname (str): Path to the ground truth file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of ground truth bounding boxes.\n",
    "    \"\"\"\n",
    "    with open(fname, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Split each line into a list of strings\n",
    "        values = line.strip().split()\n",
    "\n",
    "        # Convert numerical values to floats and keep the last element as a string\n",
    "        values = [float(val) if i < len(values) - 1 else val for i, val in enumerate(values)]\n",
    "\n",
    "        # Append the list of values to the data list\n",
    "        data.append(values)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def read_detector_file(fname):\n",
    "    \"\"\"\n",
    "    Reads detected bounding boxes from a text file.\n",
    "    Each line in the file corresponds to one detection.\n",
    "\n",
    "    Parameters:\n",
    "        fname (str): Path to the detector output file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of detected bounding boxes.\n",
    "    \"\"\"\n",
    "    with open(fname, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Split each line into a list of strings\n",
    "        values = line.strip().split()\n",
    "\n",
    "        # Convert strings to floats\n",
    "        values = [float(val) for val in values]\n",
    "\n",
    "        # Append the list of values to the data list\n",
    "        data.append(values)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Evaluation Functions\n",
    "# =====================================================\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU (Intersection over Union) between two bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "        box1 (list): Coordinates and dimensions of the first box.\n",
    "        box2 (list): Coordinates and dimensions of the second box.\n",
    "\n",
    "    Returns:\n",
    "        float: The IoU value.\n",
    "    \"\"\"\n",
    "    x1, y1, z1, w1, h1 = box1[:5]\n",
    "    x2, y2, z2, w2, h2 = box2[:5]\n",
    "\n",
    "    intersect_x = max(0, min(x1 + w1 / 2, x2 + w2 / 2) - max(x1 - w1 / 2, x2 - w2 / 2))\n",
    "    intersect_y = max(0, min(y1 + h1 / 2, y2 + h2 / 2) - max(y1 - h1 / 2, y2 - h2 / 2))\n",
    "\n",
    "    intersection = intersect_x * intersect_y\n",
    "    union = w1 * h1 + w2 * h2 - intersection\n",
    "\n",
    "    iou = intersection / max(union, 1e-10)  # Avoid division by zero\n",
    "\n",
    "    return iou\n",
    "\n",
    "def evaluate_detection(gt_boxes, dnn_boxes, iou_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Evaluates detections against ground truth boxes.\n",
    "\n",
    "    Parameters:\n",
    "        gt_boxes (list): Ground truth bounding boxes.\n",
    "        dnn_boxes (list): Detected bounding boxes.\n",
    "        iou_threshold (float): IoU threshold to consider a detection as True Positive.\n",
    "\n",
    "    Returns:\n",
    "        list: Per-detection results with confidence and TP/FP flag.\n",
    "        int: Number of False Negatives.\n",
    "        int: Total number of ground truth boxes.\n",
    "    \"\"\"\n",
    "    per_detections = []\n",
    "    matched_gt_boxes = set()\n",
    "    total_gt_boxes = len(gt_boxes)\n",
    "    \n",
    "    # Sort detections by confidence in descending order\n",
    "    dnn_boxes_sorted = sorted(dnn_boxes, key=lambda x: x[7], reverse=True)\n",
    "    \n",
    "    for det_idx, det in enumerate(dnn_boxes_sorted):\n",
    "        det_confidence = det[7]\n",
    "        det_box = det\n",
    "        is_tp = False\n",
    "\n",
    "        # Match detection to ground truth boxes\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "            if gt_idx in matched_gt_boxes:\n",
    "                continue  # Skip already matched GT boxes\n",
    "            if calculate_iou(det_box, gt_box) >= iou_threshold:\n",
    "                is_tp = True\n",
    "                matched_gt_boxes.add(gt_idx)\n",
    "                break  # Break after finding the first match\n",
    "\n",
    "        per_detections.append({'confidence': det_confidence, 'is_tp': is_tp})\n",
    "\n",
    "    num_fn = total_gt_boxes - len(matched_gt_boxes)\n",
    "    return per_detections, num_fn, total_gt_boxes\n",
    "\n",
    "def compute_ap(per_detections, total_gt_boxes):\n",
    "    \"\"\"\n",
    "    Computes Average Precision (AP) for detections.\n",
    "\n",
    "    Parameters:\n",
    "        per_detections (list): List of detections with confidence and TP/FP flag.\n",
    "        total_gt_boxes (int): Total number of ground truth boxes.\n",
    "\n",
    "    Returns:\n",
    "        float: Average Precision (AP) score.\n",
    "    \"\"\"\n",
    "    # Sort detections by confidence in descending order\n",
    "    per_detections = sorted(per_detections, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    tp_cumsum = np.cumsum([d['is_tp'] for d in per_detections])\n",
    "    fp_cumsum = np.cumsum([not d['is_tp'] for d in per_detections])\n",
    "    \n",
    "    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)\n",
    "    recalls = tp_cumsum / (total_gt_boxes + 1e-10)\n",
    "    \n",
    "    # Append sentinel values at the start and end\n",
    "    mrec = np.concatenate(([0.], recalls, [1.]))\n",
    "    mpre = np.concatenate(([0.], precisions, [0.]))\n",
    "\n",
    "    # Compute the precision envelope\n",
    "    for i in range(len(mpre) - 2, -1, -1):\n",
    "        mpre[i] = np.maximum(mpre[i], mpre[i + 1])\n",
    "\n",
    "    # Compute AP by integrating over recall levels\n",
    "    idx = np.where(mrec[1:] != mrec[:-1])[0]  # Points where recall changes\n",
    "    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n",
    "\n",
    "    return ap\n",
    "\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    \"\"\"\n",
    "    Calculates precision, recall, and F1 score.\n",
    "\n",
    "    Parameters:\n",
    "        tp (int): True Positives.\n",
    "        fp (int): False Positives.\n",
    "        fn (int): False Negatives.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Precision, Recall, F1 Score.\n",
    "    \"\"\"\n",
    "    EPSILON = 1e-10  # Avoid division by zero\n",
    "\n",
    "    precision = tp / (tp + fp + EPSILON)\n",
    "    recall = tp / (tp + fn + EPSILON)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + EPSILON)\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Visualization Function\n",
    "# =====================================================\n",
    "\n",
    "def saveHTML(pc, gt_boxes, detector_boxes, file_path=None, show_in_notebook=True, min_confidence=0.0):\n",
    "    \"\"\"\n",
    "    Creates a 3D visualization of point clouds and bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "        pc (numpy.ndarray): Point cloud data.\n",
    "        gt_boxes (list): Ground truth bounding boxes.\n",
    "        detector_boxes (list): Detected bounding boxes.\n",
    "        file_path (str): Path to save the HTML file. If None, the HTML is not saved.\n",
    "        show_in_notebook (bool): Whether to display the visualization in the notebook.\n",
    "        min_confidence (float): Minimum confidence threshold to display detections.\n",
    "    \"\"\"\n",
    "    pc = pc.T\n",
    "    data = []\n",
    "\n",
    "    # Plot the point cloud\n",
    "    data.append(go.Scatter3d(\n",
    "        x=pc[0,:], y=pc[1,:], z=pc[2,:],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=2, color='mediumturquoise', opacity=1)\n",
    "    ))\n",
    "\n",
    "    # Function to format confidence score text\n",
    "    def format_confidence_text(confidence=None):\n",
    "        if confidence is not None:\n",
    "            return f\"<b>{confidence*100:.2f}%</b>\"\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    # Define edges for the bounding boxes\n",
    "    edges = [\n",
    "        (0,1), (1,2), (2,3), (3,0),  # Bottom edges\n",
    "        (4,5), (5,6), (6,7), (7,4),  # Top edges\n",
    "        (0,4), (1,5), (2,6), (3,7)   # Vertical edges\n",
    "    ]\n",
    "\n",
    "    # Add Detector Bounding Boxes in Red with confidence scores\n",
    "    for det in detector_boxes:\n",
    "        confidence = det[7]\n",
    "        if confidence >= min_confidence:\n",
    "            bbox = BoundingBox3D(\n",
    "                x=det[0], y=det[1], z=det[2],\n",
    "                length=det[3], width=det[4], height=det[5],\n",
    "                euler_angles=[0, 0, det[6]]\n",
    "            )\n",
    "            corners = bbox.p  # shape (8,3)\n",
    "\n",
    "            # Prepare the edges of the box\n",
    "            x_edges = []\n",
    "            y_edges = []\n",
    "            z_edges = []\n",
    "\n",
    "            for edge in edges:\n",
    "                start_idx, end_idx = edge\n",
    "                x_edges += [corners[start_idx, 0], corners[end_idx, 0], None]\n",
    "                y_edges += [corners[start_idx, 1], corners[end_idx, 1], None]\n",
    "                z_edges += [corners[start_idx, 2], corners[end_idx, 2], None]\n",
    "\n",
    "            # Add the edges as lines\n",
    "            data.append(go.Scatter3d(\n",
    "                x=x_edges, y=y_edges, z=z_edges,\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=6),\n",
    "                hovertext=format_confidence_text(confidence),\n",
    "                hoverinfo='text'\n",
    "            ))\n",
    "\n",
    "            # Choose one of the top edges (e.g., edge from corner 4 to corner 5)\n",
    "            corner_start = corners[4]\n",
    "            corner_end = corners[5]\n",
    "            text_x = (corner_start[0] + corner_end[0]) / 2\n",
    "            text_y = (corner_start[1] + corner_end[1]) / 2\n",
    "            text_z = (corner_start[2] + corner_end[2]) / 2 + 0.5  # Slightly above the edge\n",
    "\n",
    "            # Add confidence score text on top of the edge\n",
    "            data.append(go.Scatter3d(\n",
    "                x=[text_x],\n",
    "                y=[text_y],\n",
    "                z=[text_z],\n",
    "                mode='text',\n",
    "                text=[format_confidence_text(confidence)],\n",
    "                textfont=dict(color='red', size=16),  # Increased font size\n",
    "                showlegend=False\n",
    "            ))\n",
    "\n",
    "    # Add Ground Truth Bounding Boxes in Green\n",
    "    for gt in gt_boxes:\n",
    "        bbox = BoundingBox3D(\n",
    "            x=gt[0], y=gt[1], z=gt[2],\n",
    "            length=gt[3], width=gt[4], height=gt[5],\n",
    "            euler_angles=[0, 0, gt[6]]\n",
    "        )\n",
    "        corners = bbox.p  # shape (8,3)\n",
    "\n",
    "        # Prepare the edges of the box\n",
    "        x_edges = []\n",
    "        y_edges = []\n",
    "        z_edges = []\n",
    "\n",
    "        for edge in edges:\n",
    "            start_idx, end_idx = edge\n",
    "            x_edges += [corners[start_idx, 0], corners[end_idx, 0], None]\n",
    "            y_edges += [corners[start_idx, 1], corners[end_idx, 1], None]\n",
    "            z_edges += [corners[start_idx, 2], corners[end_idx, 2], None]\n",
    "\n",
    "        # Add the edges as lines\n",
    "        data.append(go.Scatter3d(\n",
    "            x=x_edges, y=y_edges, z=z_edges,\n",
    "            mode='lines',\n",
    "            line=dict(color='green', width=6),\n",
    "            hoverinfo='none'\n",
    "        ))\n",
    "\n",
    "    # Define the layout with specified aesthetics\n",
    "    layout = go.Layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(nticks=40, range=[-20, 20], showbackground=True, backgroundcolor='rgb(30, 30, 30)',\n",
    "                       gridcolor='rgb(127, 127, 127)', zerolinecolor='rgb(127, 127, 127)'),\n",
    "            yaxis=dict(nticks=40, range=[-20, 20], showbackground=True, backgroundcolor='rgb(30, 30, 30)',\n",
    "                       gridcolor='rgb(127, 127, 127)', zerolinecolor='rgb(127, 127, 127)'),\n",
    "            zaxis=dict(nticks=20, range=[-1, 19], showbackground=True, backgroundcolor='rgb(30, 30, 30)',\n",
    "                       gridcolor='rgb(127, 127, 127)', zerolinecolor='rgb(127, 127, 127)'),\n",
    "            xaxis_title=\"x (meters)\",\n",
    "            yaxis_title=\"y (meters)\",\n",
    "            zaxis_title=\"z (meters)\"\n",
    "        ),\n",
    "        margin=dict(r=10, l=10, b=10, t=10),\n",
    "        paper_bgcolor='rgb(30, 30, 30)',\n",
    "        font=dict(family=\"Courier New, monospace\", color='rgb(127, 127, 127)'),\n",
    "        legend=dict(font=dict(family=\"Courier New, monospace\", color='rgb(127, 127, 127)'))\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    if show_in_notebook:\n",
    "        fig.show()\n",
    "\n",
    "    if file_path:\n",
    "        fig.write_html(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Main Processing Function\n",
    "# =====================================================\n",
    "\n",
    "def process_files(base_dir, evaluate=True, visualize=False, single_file=None, confidence_threshold=0.3, iou_threshold=0.3, show_in_notebook=False, save_visualizations=False):\n",
    "    \"\"\"\n",
    "    Processes detection and ground truth files to compute evaluation metrics and/or generate visualizations.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir (str): Base directory containing detector outputs and ground truth data.\n",
    "        evaluate (bool): Whether to perform evaluation.\n",
    "        visualize (bool): Whether to generate visualizations.\n",
    "        single_file (str): Filename to process a single file (e.g., '00001.txt'). If None, processes all files.\n",
    "        confidence_threshold (float): Confidence threshold for metrics calculation.\n",
    "        iou_threshold (float): IoU threshold for determining true positives.\n",
    "        show_in_notebook (bool): Whether to display visualizations in the notebook.\n",
    "        save_visualizations (bool): Whether to save visualizations to files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics (if evaluation is performed).\n",
    "    \"\"\"\n",
    "    detector_path = os.path.join(base_dir, 'detector_labels')\n",
    "    gt_path = os.path.join(base_dir, 'gt_test/labels')\n",
    "    npy_path = os.path.join(base_dir, 'gt_test/npy')\n",
    "    html_path = os.path.join(base_dir, 'gt_test/html')\n",
    "\n",
    "    # Prepare file list\n",
    "    if single_file:\n",
    "        file_list = [os.path.join(detector_path, single_file)]\n",
    "    else:\n",
    "        file_list = glob.glob(os.path.join(detector_path, '*.txt'))\n",
    "\n",
    "    all_detections = []\n",
    "    total_gt_boxes = 0\n",
    "\n",
    "    for file_name in file_list:\n",
    "        basename = os.path.splitext(os.path.basename(file_name))[0]\n",
    "        gt_file = os.path.join(gt_path, f\"{basename}.txt\")\n",
    "        npy_file = os.path.join(npy_path, f\"{basename}.npy\")\n",
    "\n",
    "        detector_boxes = read_detector_file(file_name)\n",
    "        gt_boxes = read_gt_file(gt_file)\n",
    "        pc = np.load(npy_file)\n",
    "\n",
    "        # Evaluate detections\n",
    "        if evaluate:\n",
    "            per_detections, num_fn, num_gt_boxes = evaluate_detection(gt_boxes, detector_boxes, iou_threshold)\n",
    "            all_detections.extend(per_detections)\n",
    "            total_gt_boxes += num_gt_boxes\n",
    "\n",
    "        # Generate visualizations\n",
    "        if visualize:\n",
    "            results_dir = os.path.join(html_path, f\"{basename}_results\")\n",
    "            if save_visualizations:\n",
    "                os.makedirs(results_dir, exist_ok=True)\n",
    "                file_path_html = os.path.join(results_dir, f\"{basename}.html\")\n",
    "            else:\n",
    "                file_path_html = None\n",
    "\n",
    "            saveHTML(\n",
    "                pc,\n",
    "                gt_boxes,\n",
    "                detector_boxes,\n",
    "                file_path=file_path_html,\n",
    "                show_in_notebook=show_in_notebook,\n",
    "                min_confidence=confidence_threshold\n",
    "            )\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    if evaluate:\n",
    "        # Compute mAP using all detections (without confidence threshold)\n",
    "        ap = compute_ap(all_detections, total_gt_boxes)\n",
    "\n",
    "        # Apply confidence threshold for precision, recall, and F1 score\n",
    "        detections_above_threshold = [d for d in all_detections if d['confidence'] >= confidence_threshold]\n",
    "        tp = sum([1 for d in detections_above_threshold if d['is_tp']])\n",
    "        fp = sum([1 for d in detections_above_threshold if not d['is_tp']])\n",
    "        fn = total_gt_boxes - tp  # FN is total ground truths minus TPs\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision, recall, f1_score = calculate_metrics(tp, fp, fn)\n",
    "\n",
    "        metrics = {\n",
    "            'Average Precision (AP)': ap,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1_score,\n",
    "            'True Positives': tp,\n",
    "            'False Positives': fp,\n",
    "            'False Negatives': fn,\n",
    "            'Total Ground Truths': total_gt_boxes\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    return None  # If evaluation is not performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Example Usage\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "# Base directory containing the necessary data\n",
    "base_dir = '/path/to/your/base_dir'\n",
    "\n",
    "# Example 1: Evaluate metrics without visualization\n",
    "metrics = process_files(\n",
    "    base_dir=base_dir,\n",
    "    evaluate=True,\n",
    "    visualize=False,\n",
    "    confidence_threshold=0.3,\n",
    "    iou_threshold=0.3\n",
    ")\n",
    "\n",
    "if metrics:\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "# Example 2: Visualize a single file in the notebook\n",
    "# process_files(\n",
    "#      base_dir=base_dir,\n",
    "#      evaluate=False,\n",
    "#      visualize=True,\n",
    "#      single_file='00001.txt',\n",
    "#      show_in_notebook=True,\n",
    "#      save_visualizations=False\n",
    "#  )\n",
    "\n",
    "# Example 3: Save visualizations for all files\n",
    "# process_files(\n",
    "#     base_dir=base_dir,\n",
    "#     evaluate=False,\n",
    "#     visualize=True,\n",
    "#     save_visualizations=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
