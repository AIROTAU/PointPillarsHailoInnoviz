{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c7a44e",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Install, imports, names\n",
    "\n",
    "Run the notebook from a virtualenv prepared like so:\n",
    "\n",
    "1. Install CUDA and Pytorch. Tested configs:\n",
    "    1. `pip install torch==1.12.1+cu113` (assuming CUDA 11.3) **OR**\n",
    "    1. `pip install torch=1.12.1+cu102` (assuming CUDA 10.2)\n",
    "1. Clone & install OpenPCDet: (tested w. commit a68aaa656 04-Apr-23) \n",
    "    ```\n",
    "     pip install -r requirements\n",
    "     pip install spconv kornia\n",
    "     python setup.py develop\n",
    "   ```\n",
    "1. Install Mayavi for 3D visualization of point clouds and 3D boxes. Installing and using Mayavi and its dependencies (PyQT5) might be tricky, especially working remotely on a server, so in the notebook code we create visual results as png files without any windows. Still, it might help to prepend the *jupyter-notebook* launch command with instructions to skip checking for gui toolkit (s.a. Qt) and, if no screen (\"headless\"), also creating and working against a virtual display, like so\n",
    "```\n",
    "ETS_TOOLKIT=null xvfb-run --server-args=\"-screen 0 1024x768x24\" jupyter notebook ...\n",
    "```\n",
    "\n",
    "1. Download pretrained PointPillar pytorch model from the link below into your *openpcdet-clone-location*:\n",
    "[https://drive.google.com/file/d/1wMxWTpU1qUoY3DsCH31WJmvJxcjFXKlm/view?usp=sharing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae8a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openpcdet clone path - replace with yours\n",
    "openpcdet_clonedir = '/home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f958e32",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy\n"
     ]
    }
   ],
   "source": [
    "cd /home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bafc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.6.0+0000000', '2.3.0+cu118')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pcdet, torch, sys\n",
    "sys.path.append(openpcdet_clonedir+'/tools/')\n",
    "pcdet.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658f6bc",
   "metadata": {},
   "source": [
    "Make sure you are in the the currect folder (pointpillars):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0553a41",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/3D_Object_Detector/mmlab_hailo/Hailo-Innoviz-HW-Offload/model_compilation/pointpillars\n"
     ]
    }
   ],
   "source": [
    "cd /home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/3D_Object_Detector/mmlab_hailo/Hailo-Innoviz-HW-Offload/model_compilation/pointpillars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74c9a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'openpcdet2hailo_utils' from '/home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/3D_Object_Detector/mmlab_hailo/Hailo-Innoviz-HW-Offload/model_compilation/pointpillars/openpcdet2hailo_utils.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pcdet.config import cfg, cfg_from_yaml_file\n",
    "from pcdet.models import build_network\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "import os\n",
    "from importlib import reload\n",
    "import openpcdet2hailo_utils as ohu; reload(ohu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcc724",
   "metadata": {},
   "source": [
    "Specify the path to the relevant model's .yaml file and the path to the model's .pth weights file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433a1a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yaml_name = openpcdet_clonedir+'/tools/cfgs/custom_models/point_pillar_best.yaml'\n",
    "har_name = 'pp_bev_w_head.har'\n",
    "hef_name = har_name.replace('.har', '.hef')\n",
    "logger = common_utils.create_logger()\n",
    "\n",
    "# (!) pre-trained model:\n",
    "pth_name ='/home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/3D_Object_Detector/Evaluation/pointpillar_evaluations/models_outputs/point_pillar_best/default/ckpt/checkpoint_epoch_80.pth'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42477a3",
   "metadata": {},
   "source": [
    "Here below I load the point cloud samples I have.\n",
    "\n",
    "This can be done in any way - the important thing is to have a directory under 'pointpillar' folder with the point clouds. By defualt it expects .npy files but you can also try other formats just remember to change the pc_file_extention variable below. \n",
    "\n",
    "Also - specify the path to the point cloud sample (under sample_pointclouds) and give one point cloud example (under demo_pointcloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be722597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (!) get sample pointclouds (..running once..) \n",
    "# specify the path to the pointclouds:\n",
    "file_names_path = '/home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/data/custom/ImageSets/val.txt'\n",
    "source_folder_npy = '/home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/data/custom/points'\n",
    "destination_folder = '/home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/3D_Object_Detector/mmlab_hailo/Hailo-Innoviz-HW-Offload/model_compilation/pointpillars/results'\n",
    "pc_file_extention= '.npy'\n",
    "\n",
    "# creates the samples in a new folder named \"pc_samples\"\n",
    "ohu.get_pc_samples(file_names_path, source_folder_npy, destination_folder)\n",
    "\n",
    "sample_pointclouds = './pc_samples/testing/innoviz/'\n",
    "demo_pointcloud = sample_pointclouds+'00001.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a60812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(cfg, pth_name, demo_dataset):    \n",
    "    model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=demo_dataset)\n",
    "    model.load_params_from_file(filename=pth_name, logger=logger, to_cpu=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def cfg_from_yaml_file_wrap(yaml_name, cfg):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(openpcdet_clonedir+'/tools/')\n",
    "    cfg_from_yaml_file(yaml_name, cfg)\n",
    "    os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12a2e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tauproj1/Innoviz_Project/ppenv/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hailo_sdk_client\n",
    "print(hailo_sdk_client.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497739e",
   "metadata": {},
   "source": [
    "# Running end2end with 2D part offloaded to Hailo HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfaf0e0",
   "metadata": {},
   "source": [
    "```\n",
    "We will be using HailoRT asynchronous send/receive to demonstrate readiness for a fully pipelined processing.\n",
    "\n",
    "Therefore, we integrate the Torch and Hailo parts differently from what we did for the emulation testing. We rip off some code from original forward() to build two torch models which encapsulate everything that happens before and after the module offloaded to Hailo.  See the code for 'PP_Pre_Bev_w_Head', 'PP_Post_Bev_w_Head' classes in the accompanying utils file.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d484e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ohu);  # PP_Pre_Bev_w_Head, PP_Post_Bev_w_Head, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f58851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ================================================\n",
    "    Standard wrapping of HailoRT API with send/receive processes, \n",
    "    similar to other Hailo examples (or HailoRT tutorial)\n",
    "    ================================================\n",
    "\"\"\"\n",
    "from multiprocessing import Process, Queue\n",
    "from hailo_platform import (HEF, PcieDevice, VDevice, HailoStreamInterface, ConfigureParams,\n",
    " InputVStreamParams, OutputVStreamParams, InputVStreams, OutputVStreams, FormatType)\n",
    "\n",
    "def send_from_queue(configured_network, read_q, num_images, start_time):\n",
    "    \"\"\" Bridging a queue into Hailo platform FEED. To be run as a separate process. \n",
    "        Reads (preprocessed) images from a given queue, and sends them serially to Hailo platform.        \n",
    "    \"\"\"    \n",
    "    configured_network.wait_for_activation(1000)\n",
    "    vstreams_params = InputVStreamParams.make(configured_network, quantized=False, format_type=FormatType.FLOAT32)\n",
    "    print('Starting sending input images to HW inference...\\n')\n",
    "    with InputVStreams(configured_network, vstreams_params) as vstreams:\n",
    "        vstream_to_buffer = {vstream: np.ndarray([1] + list(vstream.shape), dtype=vstream.dtype) for vstream in vstreams}\n",
    "        for i in range(num_images):\n",
    "            hailo_inp = read_q.get()\n",
    "            for vstream, _ in vstream_to_buffer.items():                                \n",
    "                vstream.send(hailo_inp)\n",
    "            print(f'sent img #{i}')\n",
    "    print(F'Finished send after {(time.time()-start_time) :.1f}')\n",
    "    return 0\n",
    "\n",
    "def recv_to_queue(configured_network, write_q, num_images, start_time):\n",
    "    \"\"\" Bridging Hailo platform OUTPUT into a queue. To be run as a separate process. \n",
    "        Reads output data from Hailo platform and sends them serially to a given queue.\n",
    "    \"\"\"\n",
    "    configured_network.wait_for_activation(1000)\n",
    "    vstreams_params = OutputVStreamParams.make_from_network_group(configured_network, quantized=False, format_type=FormatType.FLOAT32)\n",
    "    print('Starting receving HW inference output..\\n')\n",
    "    with OutputVStreams(configured_network, vstreams_params) as vstreams:\n",
    "        # print('vstreams_params', vstreams_params)\n",
    "        for i in range(num_images):            \n",
    "            hailo_out = {vstream.name: np.expand_dims(vstream.recv(), 0) for vstream in vstreams}    \n",
    "            \n",
    "            print(\"hailo_out keys:\", hailo_out.keys())\n",
    "                      \n",
    "            write_q.put(hailo_out)\n",
    "            print(f'received img #{i}')\n",
    "    print(F'Finished recv after {time.time()-start_time :.1f}')\n",
    "    return 0\n",
    "\n",
    "\"\"\" ==================================\n",
    "    Some final wrapping of pre and post \n",
    "    ==================================\n",
    "\"\"\"\n",
    "# Modify generate_data_dicts to include sample_name\n",
    "def generate_data_dicts(demo_dataset, num_images, pp_pre_bev_w_head):\n",
    "    for idx, data_dict in enumerate(demo_dataset):\n",
    "        if idx > num_images:\n",
    "            break\n",
    "        data_dict = demo_dataset.collate_batch([data_dict])\n",
    "        ohu.load_data_to_CPU(data_dict)\n",
    "        # Add sample_name to data_dict with only the file name\n",
    "        data_dict['sample_name'] = os.path.basename(demo_dataset.sample_file_list[idx])\n",
    "        # ------ (!) Applying torch PRE-processing -------\n",
    "        data_dict = pp_pre_bev_w_head.forward(data_dict)\n",
    "        # ------------------------------------------------\n",
    "        logger.info(f'preprocessed sample #{idx}')\n",
    "        yield data_dict\n",
    "\n",
    "# Pass the sample_name through the pipeline\n",
    "def generate_hailo_inputs(demo_dataset, num_images, pp_pre_bev_w_head):\n",
    "    \"\"\" generator-style encapsulation for preprocessing inputs for Hailo HW feed\n",
    "    \"\"\"\n",
    "    for data_dict in generate_data_dicts(demo_dataset, num_images, pp_pre_bev_w_head):\n",
    "        spatial_features = data_dict['spatial_features']\n",
    "        spatial_features_hailoinp = np.transpose(spatial_features.cpu().detach().numpy(), (0, 2, 3, 1))\n",
    "        yield data_dict, spatial_features_hailoinp\n",
    "\n",
    "# Attach the sample_name to the prediction dictionaries in post_proc_from_queue\n",
    "def post_proc_from_queue(recv_queue, num_images, pp_post_bev_w_head,\n",
    "                         output_layers_order=['model/concat1', 'model/conv19', 'model/conv18', 'model/conv20']):\n",
    "    results = []\n",
    "    for i in range(num_images):\n",
    "        t_ = time.time()\n",
    "        while(recv_queue.empty() and time.time()-t_ < 3):\n",
    "            time.sleep(0.01)\n",
    "        if recv_queue.empty():\n",
    "            print(\"RECEIVE TIMEOUT!\")\n",
    "            break\n",
    "        hailo_out = recv_queue.get(0)\n",
    "        bev_out = (hailo_out[lname] for lname in output_layers_order)\n",
    "        \n",
    "        # ------ (!) Applying torch POST-processing -------\n",
    "        pred_dicts, _ = pp_post_bev_w_head(bev_out)\n",
    "        # ------------------------------------------------\n",
    "        # Add sample_name to each prediction dictionary\n",
    "        sample_name = recv_queue.sample_names[i]\n",
    "        for pred_dict in pred_dicts:\n",
    "            pred_dict['sample_name'] = sample_name\n",
    "        results.append(pred_dicts)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db9eeea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time, onnxruntime\n",
    "\n",
    "# data_source = demo_pointcloud  # replace by a folder for a more serious test\n",
    "# num_images = 1\n",
    "# # pc_samples = './pc_samples/testing/innoviz/'\n",
    "# # data_source = pc_samples\n",
    "# # num_images = 51\n",
    "\n",
    "# cfg_from_yaml_file_wrap(yaml_name, cfg)\n",
    "# logger = common_utils.create_logger()\n",
    "# demo_dataset = ohu.DemoDataset(\n",
    "#     dataset_cfg=cfg.DATA_CONFIG, class_names=cfg.CLASS_NAMES, training=False,\n",
    "#     root_path=Path(data_source), ext=pc_file_extention, logger=logger\n",
    "#     )\n",
    "# model = get_model(cfg, pth_name, demo_dataset)\n",
    "\n",
    "# # Library creates the anchors in cuda by defalt (applying .cuda() in internal implementation)\n",
    "# model.dense_head.anchors = [anc.cpu() for anc in model.dense_head.anchors]\n",
    "\n",
    "# \"\"\" (!) Slicing off the torch model all that happens before and after Hailo\n",
    "# \"\"\"\n",
    "\n",
    "# pp_pre_bev_w_head = ohu.PP_Pre_Bev_w_Head(model)\n",
    "# pp_post_bev_w_head = ohu.PP_Post_Bev_w_Head(model)\n",
    "    \n",
    "# # Adjusting the processing loop to handle sample_name\n",
    "# with VDevice() as target:\n",
    "#     hef = HEF(hef_name)\n",
    "#     configure_params = ConfigureParams.create_from_hef(hef, interface=HailoStreamInterface.PCIe)\n",
    "#     network_group = target.configure(hef, configure_params)[0]\n",
    "#     network_group_params = network_group.create_params()\n",
    "#     recv_queue = Queue()\n",
    "#     send_queue = Queue()\n",
    "#     start_time = time.time()\n",
    "#     results = []\n",
    "#     hw_send_process = Process(target=send_from_queue, args=(network_group, send_queue, num_images, start_time))\n",
    "#     hw_recv_process = Process(target=recv_to_queue, args=(network_group, recv_queue, num_images, start_time))\n",
    "\n",
    "#     # List to keep track of sample names\n",
    "#     sample_names = []\n",
    "\n",
    "#     with network_group.activate(network_group_params):\n",
    "#         hw_recv_process.start()\n",
    "#         hw_send_process.start()\n",
    "\n",
    "#         for data_dict, hailo_inp in generate_hailo_inputs(demo_dataset, num_images, pp_pre_bev_w_head):\n",
    "#             tik = time.time()\n",
    "\n",
    "#             send_queue.put(hailo_inp)\n",
    "#             # Add sample_name to the list\n",
    "#             sample_names.append(data_dict['sample_name'])\n",
    "\n",
    "#         # # Attach sample names to the queue\n",
    "#         recv_queue.sample_names = sample_names\n",
    "\n",
    "#         results = post_proc_from_queue(recv_queue, num_images, pp_post_bev_w_head)\n",
    "\n",
    "#         tok = time.time()\n",
    "#         elapsed_time = tok - tik\n",
    "#         print(f\"Elapsed time: {elapsed_time/num_images} seconds\")\n",
    "                             \n",
    "#     hw_recv_process.join(10)\n",
    "#     hw_send_process.join(10)\n",
    "    \n",
    "#     pred_dicts = results[-1]\n",
    "#     print(pred_dicts[0]['pred_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1037db53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tauproj1/Innoviz_Project/ppenv/lib/python3.8/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "2024-10-22 15:02:30,537   INFO  ==> Loading parameters from checkpoint /home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/3D_Object_Detector/Evaluation/pointpillar_evaluations/models_outputs/point_pillar_best/default/ckpt/checkpoint_epoch_80.pth to CPU\n",
      "2024-10-22 15:02:30,537   INFO  ==> Loading parameters from checkpoint /home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/3D_Object_Detector/Evaluation/pointpillar_evaluations/models_outputs/point_pillar_best/default/ckpt/checkpoint_epoch_80.pth to CPU\n",
      "2024-10-22 15:02:30,560   INFO  ==> Checkpoint trained from version: pcdet+0.6.0+0000000\n",
      "2024-10-22 15:02:30,560   INFO  ==> Checkpoint trained from version: pcdet+0.6.0+0000000\n",
      "2024-10-22 15:02:30,565   INFO  ==> Done (loaded 127/127)\n",
      "2024-10-22 15:02:30,565   INFO  ==> Done (loaded 127/127)\n",
      "[HailoRT] [warning] Desc page size value (1024) is not optimal for performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting receving HW inference output..\n",
      "\n",
      "Starting sending input images to HW inference...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 15:02:30,850   INFO  preprocessed sample #0\n",
      "2024-10-22 15:02:30,850   INFO  preprocessed sample #0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent img #0\n",
      "Finished send after 0.3\n",
      "hailo_out keys: dict_keys(['model/conv20', 'model/conv19', 'model/conv18', 'model/concat1'])\n",
      "received img #0\n",
      "Finished recv after 0.4\n",
      "(1, 248, 216, 6) <class 'numpy.ndarray'> (1, 248, 216, 42)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Attach sample names to the queue\u001b[39;00m\n\u001b[1;32m     55\u001b[0m recv_queue\u001b[38;5;241m.\u001b[39msample_names \u001b[38;5;241m=\u001b[39m sample_names\n\u001b[0;32m---> 57\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpost_proc_from_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecv_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpp_post_bev_w_head\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Stop timing after processing\u001b[39;00m\n\u001b[1;32m     60\u001b[0m tok \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[11], line 89\u001b[0m, in \u001b[0;36mpost_proc_from_queue\u001b[0;34m(recv_queue, num_images, pp_post_bev_w_head, output_layers_order)\u001b[0m\n\u001b[1;32m     86\u001b[0m bev_out \u001b[38;5;241m=\u001b[39m (hailo_out[lname] \u001b[38;5;28;01mfor\u001b[39;00m lname \u001b[38;5;129;01min\u001b[39;00m output_layers_order)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# ------ (!) Applying torch POST-processing -------\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m pred_dicts, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpp_post_bev_w_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbev_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Add sample_name to each prediction dictionary\u001b[39;00m\n\u001b[1;32m     92\u001b[0m sample_name \u001b[38;5;241m=\u001b[39m recv_queue\u001b[38;5;241m.\u001b[39msample_names[i]\n",
      "File \u001b[0;32m~/Innoviz_Project/ppenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Innoviz_Project/ppenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Innoviz_Project/OpenPCDet-master_New_copy/3D_Object_Detector/mmlab_hailo/Hailo-Innoviz-HW-Offload/model_compilation/pointpillars/openpcdet2hailo_utils.py:219\u001b[0m, in \u001b[0;36mPP_Post_Bev_w_Head.forward\u001b[0;34m(self, bev_out)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Here's the unavoidable cuda part:    \u001b[39;00m\n\u001b[1;32m    218\u001b[0m cuda_data_dict \u001b[38;5;241m=\u001b[39m {k: (v\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;241m==\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;28;01melse\u001b[39;00m v) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m data_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 219\u001b[0m pred_dicts, recall_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpp_full_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda_data_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_dicts, recall_dicts\n",
      "File \u001b[0;32m~/Innoviz_Project/OpenPCDet-master_New_copy/pcdet/models/detectors/detector3d_template.py:215\u001b[0m, in \u001b[0;36mDetector3DTemplate.post_processing\u001b[0;34m(self, batch_dict)\u001b[0m\n\u001b[1;32m    212\u001b[0m cls_preds \u001b[38;5;241m=\u001b[39m batch_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_cls_preds\u001b[39m\u001b[38;5;124m'\u001b[39m][batch_mask]\n\u001b[1;32m    214\u001b[0m src_cls_preds \u001b[38;5;241m=\u001b[39m cls_preds\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m cls_preds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_class]\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_preds_normalized\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    218\u001b[0m     cls_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(cls_preds)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time, onnxruntime\n",
    "\n",
    "data_source = demo_pointcloud  # replace by a folder for a more serious test\n",
    "num_images = 1\n",
    "# pc_samples = './pc_samples/testing/innoviz/'\n",
    "# data_source = pc_samples\n",
    "# num_images = 51\n",
    "\n",
    "cfg_from_yaml_file_wrap(yaml_name, cfg)\n",
    "logger = common_utils.create_logger()\n",
    "demo_dataset = ohu.DemoDataset(\n",
    "    dataset_cfg=cfg.DATA_CONFIG, class_names=cfg.CLASS_NAMES, training=False,\n",
    "    root_path=Path(data_source), ext=pc_file_extention, logger=logger\n",
    ")\n",
    "model = get_model(cfg, pth_name, demo_dataset)\n",
    "\n",
    "# Library creates the anchors in cuda by default (applying .cuda() in internal implementation)\n",
    "model.dense_head.anchors = [anc.cpu() for anc in model.dense_head.anchors]\n",
    "\n",
    "\"\"\" (!) Slicing off the torch model all that happens before and after Hailo\n",
    "\"\"\"\n",
    "\n",
    "pp_pre_bev_w_head = ohu.PP_Pre_Bev_w_Head(model)\n",
    "pp_post_bev_w_head = ohu.PP_Post_Bev_w_Head(model)\n",
    "    \n",
    "# Adjusting the processing loop to handle sample_name\n",
    "with VDevice() as target:\n",
    "    hef = HEF(hef_name)\n",
    "    configure_params = ConfigureParams.create_from_hef(hef, interface=HailoStreamInterface.PCIe)\n",
    "    network_group = target.configure(hef, configure_params)[0]\n",
    "    network_group_params = network_group.create_params()\n",
    "    recv_queue = Queue()\n",
    "    send_queue = Queue()\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    hw_send_process = Process(target=send_from_queue, args=(network_group, send_queue, num_images, start_time))\n",
    "    hw_recv_process = Process(target=recv_to_queue, args=(network_group, recv_queue, num_images, start_time))\n",
    "\n",
    "    # List to keep track of sample names\n",
    "    sample_names = []\n",
    "\n",
    "    with network_group.activate(network_group_params):\n",
    "        hw_recv_process.start()\n",
    "        hw_send_process.start()\n",
    "\n",
    "        # Start timing before the loop\n",
    "        tik = time.time()\n",
    "\n",
    "        for data_dict, hailo_inp in generate_hailo_inputs(demo_dataset, num_images, pp_pre_bev_w_head):\n",
    "            send_queue.put(hailo_inp)\n",
    "            # Add sample_name to the list\n",
    "            sample_names.append(data_dict['sample_name'])\n",
    "\n",
    "        # Attach sample names to the queue\n",
    "        recv_queue.sample_names = sample_names\n",
    "\n",
    "        results = post_proc_from_queue(recv_queue, num_images, pp_post_bev_w_head)\n",
    "\n",
    "        # Stop timing after processing\n",
    "        tok = time.time()\n",
    "        elapsed_time = tok - tik\n",
    "        average_time_per_image = elapsed_time / num_images\n",
    "        inference_rate_hz = num_images / elapsed_time\n",
    "\n",
    "        print(f\"Total elapsed time: {elapsed_time:.4f} seconds\")\n",
    "        print(f\"Average time per image: {average_time_per_image:.4f} seconds\")\n",
    "        print(f\"Inference rate: {inference_rate_hz:.2f} Hz\")\n",
    "                             \n",
    "    hw_recv_process.join(10)\n",
    "    hw_send_process.join(10)\n",
    "    \n",
    "    pred_dicts = results[-1]\n",
    "    print(pred_dicts[0]['pred_scores'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dfd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d57b69",
   "metadata": {},
   "source": [
    "Creating a 'prediction' folder under output_path containing the results in .txt format for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25a9b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(results, output_path):\n",
    "    # Create the prediction folder if it doesn't exist\n",
    "    prediction_folder = os.path.join(output_path, \"predictions\")\n",
    "    os.makedirs(prediction_folder, exist_ok=True)\n",
    "    \n",
    "    for sample_predictions in results:\n",
    "        for prediction in sample_predictions:\n",
    "            sample_name = prediction['sample_name']\n",
    "            file_name = os.path.splitext(sample_name)[0] + \".txt\"\n",
    "            file_path = os.path.join(prediction_folder, file_name)\n",
    "            \n",
    "            with open(file_path, 'w') as file:\n",
    "                pred_boxes = prediction['pred_boxes'].cpu().numpy()\n",
    "                pred_scores = prediction['pred_scores'].cpu().numpy()\n",
    "                for box, score in zip(pred_boxes, pred_scores):\n",
    "                    line = ' '.join(map(str, box)) + f' {score}\\n'\n",
    "                    file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bfc58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/home/tauproj1/Innoviz_Project/OpenPCDet-master_New_copy/3D_Object_Detector/mmlab_hailo/Hailo-Innoviz-HW-Offload/model_compilation/pointpillars'  # Specify your desired output path here\n",
    "save_predictions(results, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
